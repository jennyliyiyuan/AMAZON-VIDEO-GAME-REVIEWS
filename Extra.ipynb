
# data loading

# In[1]:
# import libraries
# download via API/Internet
import os
import json
import gzip
from urllib.request import urlopen

# data handling
import numpy as np
import pandas as pd
# data visualization
import seaborn as sns
# pytorch
import torch
from torch import nn
from torch import optim

# check if GPU is available (better to check it at the start)
# device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print('Using gpu: %s ' % torch.cuda.is_available())
# download the data from link
import requests
import os

# URL of the file you want to download
url = "https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_v2/categoryFilesSmall/Video_Games_5.json.gz"

# Local path where you want to save the downloaded file
local_filename = "Video_Games.json.gz"

# Check if the file already exists, if yes, remove it
if os.path.exists(local_filename):
    os.remove(local_filename)

# Download the file
response = requests.get(url)
with open(local_filename, 'wb') as f:
    f.write(response.content)

print(f"File '{local_filename}' downloaded successfully.")

# load the meta data
data = []
with gzip.open(local_filename, 'rt') as f:  # 'rt' for text mode
    for l in f:
        data.append(json.loads(l.strip()))

# total length of the list, this number equals the total number of products
print(len(data))

# first row of the list
print(data[0])


# In[2]:


def parse(path):
  g = gzip.open(path, 'rb')
  for l in g:
    yield json.loads(l)

def getDF(path):
  i = 0
  df = {}
  for d in parse(path):
    df[i] = d
    i += 1
  return pd.DataFrame.from_dict(df, orient='index')

df = getDF('Video_Games.json.gz')


# In[3]:


df.head


# In[4]:


csv_file_path = "/Users/jennyli/Desktop/Amazon.csv"
df.to_csv(csv_file_path, index=False)
print(f"DataFrame successfully converted and saved to {csv_file_path}")


# In[5]:


# start here


# In[6]:


# data handling
import numpy as np
import pandas as pd
# data visualization
import seaborn as sns
# pytorch
import torch
from torch import nn
from torch import optim


# In[7]:


# Read the CSV file into a DataFrame
df = pd.read_csv('/Users/jennyli/Desktop/Amazon.csv')
df


# In[8]:


print(f"Length of DataFrame: {len(df)}")


# data clean

# In[9]:


# 1. Removing duplicates
df = df.drop_duplicates()
print(f"Length of DataFrame: {len(df)}")


# In[10]:


# 2. Handling missing values
df.fillna(0, inplace=True) # Replace 0 with your desired placeholde


# In[11]:


#3. Converting data types
df['reviewTime'] = pd.to_datetime(df['reviewTime'], errors='coerce')


# In[12]:


# Check for missing values
print('Missing values:\n', df.isnull().sum())
# Print summary statistics
print('Summary statistics:\n', df.describe())


# In[13]:


import numpy as np
import pandas as pd
get_ipython().system('pip install nltk')
import nltk
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
from sklearn.metrics import f1_score
nltk.download('punkt') 
nltk.download('stopwords') 
from nltk.corpus import stopwords
stop = set(stopwords.words('english'))
from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize
from nltk.stem import PorterStemmer
ps = PorterStemmer()
from collections import defaultdict
freq = defaultdict(int)
import re
import matplotlib.pyplot as plt
from tqdm import tqdm
from math import log

import pickle
from sklearn.linear_model import LogisticRegressionCV
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
get_ipython().system(' pip install wordcloud')
from wordcloud import WordCloud, STOPWORDS
from PIL import Image


# In[14]:


unique_overallrating_values = df['overall'].unique()
unique_overallrating_values 


# In[15]:


clean_data = df.copy()  # Add parentheses to call the copy method
csv_file_path = "/Users/jennyli/Desktop/clean_data.csv"
clean_data.to_csv(csv_file_path, index=False)
print(f"DataFrame successfully converted and saved to {csv_file_path}")


# In[16]:


# Remove all the links
clean_data['reviewText'] = clean_data['reviewText'].str.split('https:')


# In[17]:


# Remove all non-alphanumeric characters from the text list
# Select the text part of the list
reviewText = [str(i[0]) if isinstance(i, list) and len(i) > 0 else '' for i in clean_data.reviewText]
clean_data['reviewText'] = reviewText

string = r'[A-Za-z0-9 ]'
trim_list=[]

for row in reviewText:
    s=''
    for letter in row:
        if bool(re.match(string, letter)):
            s+=letter
    trim_list.append(s)


# In[18]:
# Remove the non-printing characters from text
rep_list = ['\U0001fae1', '\\n', '@', '#', '\xa0', '***', 'https', 'nhttps']

for i in trim_list:
    for j in rep_list:
        if j in i:
            i.replace(j,'')


# In[19]:


import matplotlib.pyplot as plt
import seaborn as sns
get_ipython().system(' pip install wordcloud')
from wordcloud import WordCloud, STOPWORDS
from PIL import Image

get_ipython().run_line_magic('matplotlib', 'inline')


# In[20]:


# Map sentiment values based on 'overall' column
clean_data['labels'] = clean_data['overall'].map({1: 'negative', 2: 'negative', 3: 'neutral', 4: 'positive', 5: 'positive'})


# In[21]:


clean_data.head()


# In[22]:
####plot the contribution of all ratings

import seaborn as sns

# Count the frequency of each overall category
label_counts = clean_data['overall'].value_counts()

# Create a count plot using Seaborn
plt.figure(figsize=(10, 6))
sns.countplot(data=clean_data, x='overall', order=label_counts.index)

# Set labels for the plot
plt.title('Frequency of Each Category in rating')
plt.xlabel('Category')
plt.ylabel('Count')

# Rotate x-axis labels for better visibility if needed
plt.xticks(rotation=45, ha='right')

# Show the plot
plt.show()


# In[23]:
#########plot the contribution of sentiment

# Count the frequency of labels category
label_counts = clean_data['labels'].value_counts()
plt.figure(figsize=(10, 6))
sns.countplot(data=clean_data, x='labels', order=label_counts.index)
plt.title('Frequency of Each Category in "labels" Column')
plt.xlabel('Category')
plt.ylabel('Count')

plt.xticks(rotation=45, ha='right')

plt.show()


# In[ ]:
#########  WordCloud visualization
from wordcloud import WordCloud
from nltk.corpus import stopwords
from nltk import pos_tag
from nltk.tokenize import word_tokenize
import matplotlib.pyplot as plt
import pandas as pd
import nltk

nltk.download('stopwords')
nltk.download('punkt')

# In[ ]:
# for overall review text 

# Concatenate all text in the 'reviewText' column
all_text = ' '.join(clean_data['reviewText'])

# Tokenize and tag part-of-speech
tokens = word_tokenize(all_text)
tags = pos_tag(tokens)

# Extract nouns
nouns = [word for word, pos in tags if pos.startswith('N')]

# Define custom stopwords
custom_stopwords = set(['games', 'game', 'play', 'good', 'thing', 'one', 'love', 'someone', 'something', 'everything', 'anything', 'things'] + stopwords.words('english'))

# Create a WordCloud object
wordcloud = WordCloud(width=800, height=400,
                      background_color='white',
                      stopwords=custom_stopwords,
                      min_font_size=10).generate(' '.join(nouns))

# Plot the WordCloud
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud for Nouns in ReviewText')
plt.show()


# In[ ]:
# for different sentiment categories
# Filter reviews based on labels
positive_reviews = clean_data[clean_data['labels'] == 'positive']
negative_reviews = clean_data[clean_data['labels'] == 'negative']
neutral_reviews = clean_data[clean_data['labels'] == 'neutral']

def generate_wordcloud(data, title):
    # Concatenate all text in the 'reviewText' column
    all_text = ' '.join(data['reviewText'])

    # Tokenize and tag part-of-speech
    tokens = word_tokenize(all_text)
    tags = pos_tag(tokens)

    # Extract nouns
    nouns = [word for word, pos in tags if pos.startswith('N')]

    # Define custom stopwords
    custom_stopwords = set(['games', 'game', 'play', 'good','bad', 'thing', 'one', 'love', 'someone', 'something', 'everything', 'anything', 'things'] + stopwords.words('english'))

    # Create a WordCloud object
    wordcloud = WordCloud(width=800, height=400,
                          background_color='white',
                          stopwords=custom_stopwords,
                          min_font_size=10).generate(' '.join(nouns))

    # Plot the WordCloud
    plt.figure(figsize=(10, 6))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(f'Word Cloud for Nouns in {title} Reviews')
    plt.show()

# Generate word clouds for each sentiment
generate_wordcloud(positive_reviews, 'Positive')
generate_wordcloud(negative_reviews, 'Negative')
generate_wordcloud(neutral_reviews, 'Neutral')

# In[ ]:
# Top 20 "asin" based on the sum of "overall" scores
clean_data['overall'] = pd.to_numeric(clean_data['overall'])
top_asin_overall = clean_data.groupby('asin')['overall'].sum().nlargest(20).index

print("Top 20 'asin' based on the sum of 'overall' scores:")
print(top_asin_overall)

# Top 20 "asin" with the largest number of "positive" labels
top_asin_positive = clean_data[clean_data['labels'] == 'positive'].groupby('asin').size().nlargest(20).index

print("\nTop 20 'asin' with the largest number of 'positive' labels:")
print(top_asin_positive)


        
# In[ ]:

 #########find important words topics of most popular game with highest rating


clean_data['overall'] = pd.to_numeric(clean_data['overall'], errors='coerce')

# Filter out non-numeric values in 'like' and 'vote'
clean_data['vote'] = pd.to_numeric(clean_data['vote'], errors='coerce')

# Top 10 "asin" based on the sum of "overall" scores
top_asin_overall = clean_data.groupby('asin')['overall'].sum().nlargest(10).index

# Create a DataFrame for the top 20 "asin"
top_asin_df = clean_data[clean_data['asin'].isin(top_asin_overall)].copy()

# Filter out non-numeric values in 'like' and 'vote' for the top 20 "asin"
top_asin_df['vote'] = pd.to_numeric(top_asin_df['vote'], errors='coerce')

# Calculate average overall rating, average like, average vote for each "asin"
top_asin_stats = top_asin_df.groupby('asin').agg({
    'overall': 'mean',
    'vote': 'mean',
    'style': lambda x: x.value_counts().idxmax()  # Most common style
}).reset_index()

# Rename the columns
top_asin_stats.columns = ['asin', 'avg_overall', 'avg_vote', 'most_common_style']

# Display the DataFrame
top_asin_stats.head()



# Top 10 "asin" based on the highest number of unique reviewerIDs
top_asin_unique_reviewerID = clean_data.groupby('asin')['reviewerID'].nunique().nlargest(10)

# Display the result in descending order
print(top_asin_unique_reviewerID)

# Top 10 "asin" based on the highest ratings
top_asin_unique_ratings = clean_data.groupby('asin')['overall'].sum().nlargest(10)

print(top_asin_unique_ratings)
# In[ ]:
# Filter data for the specific 'asin'
specific_asin_data = clean_data[clean_data['asin'] == 'B00JK00S0S']

# Calculate the percentage of positive reviews
positive_review_percentage = (specific_asin_data[specific_asin_data['labels'] == 'positive'].shape[0] / specific_asin_data.shape[0]) * 100

# Display the result
print(f"The positive review percentage for asin B00JK00S0S is: {positive_review_percentage:.2f}%")

 
##### we find that the product "B00JK00S0S " has highest reviews and highest ratings. and its positive review percentage is: 91.32%. in next follow steps, we want to try to use different method to investigate important words of its positive topics and find the reason why it is popular.

# In[ ]:
    
!pip install scikit-learn matplotlib    
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.tree import DecisionTreeClassifier
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report
# In[ ]:
# Filter data for 'positive' reviews with highest 'overall' sum
positive_asin = clean_data[clean_data['labels'] == 'positive'].groupby('asin')['overall'].sum().idxmax()
positive_reviews = clean_data[(clean_data['asin'] == positive_asin) & (clean_data['labels'] == 'positive')]

# Feature space (X) and labels (Y)
X = []
Y = []

for ind in positive_reviews.index:
    terms = [word for word in positive_reviews['reviewText'][ind].split()]
    X.append(' '.join(terms))
    
    labels = [word for word in positive_reviews['labels'][ind].split()]
    Y.append(labels)

# Display the processed X and Y
print("Processed X:")
print(X)
print("\nProcessed Y:")
print(Y)

# In[ ]:
import nltk
nltk.download('wordnet')
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk import pos_tag
import string
# In[ ]:
# 1. TF-IDF to find key topics words
# Define custom stopwords and additional words to remove
custom_stopwords = set(['games', 'game'])
additional_stopwords = set(stopwords.words('english')).union(custom_stopwords)

def preprocess_text(text):
    # Tokenize the text
    tokens = word_tokenize(text)
    
    # Remove punctuation and convert to lowercase
    tokens = [word.lower() for word in tokens if word.isalpha() and word not in string.punctuation]
    
    # Filter out stopwords and additional words
    tokens = [word for word in tokens if word not in additional_stopwords]
    
    # Part-of-speech tagging
    tags = pos_tag(tokens)
    
    # Keep only nouns
    nouns = [word for word, pos in tags if pos.startswith('N')]
    
    return ' '.join(nouns)

# Apply preprocessing to each document in X
X_preprocessed = [preprocess_text(doc) for doc in X]

# Create TF-IDF vectorizer
vectorizer = TfidfVectorizer(stop_words='english')

# Fit and transform the preprocessed text data
X_tfidf = vectorizer.fit_transform(X_preprocessed)

# Get feature names (words) from the vectorizer
feature_names = vectorizer.get_feature_names_out()

# Get TF-IDF values for each word in the first document
tfidf_values = X_tfidf[0].todense().tolist()[0]

# Combine feature names with their corresponding TF-IDF values
word_tfidf_pairs = list(zip(feature_names, tfidf_values))

# Sort the pairs by TF-IDF values in descending order
word_tfidf_pairs.sort(key=lambda x: x[1], reverse=True)

# Print the top N important words and their TF-IDF values
top_n_words = 30
for word, tfidf in word_tfidf_pairs[:top_n_words]:
    print(f'{word}: {tfidf}')



# In[ ]:
# 2. LDA find topics words
    
import nltk
import numpy as np
from gensim import corpora, models
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# Ensure required NLTK datasets are downloaded
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')

def build_lda_model(corpus, dictionary, num_topics):
    lda_model = models.LdaModel(
        corpus=corpus,
        id2word=dictionary,
        num_topics=num_topics,
        random_state=42,
        update_every=1,
        chunksize=100,
        passes=10,
        alpha='auto'
    )
    return lda_model

def calculate_cosine_similarity(topic1, topic2):
    topic1_vector = np.array([val for _, val in topic1])
    topic2_vector = np.array([val for _, val in topic2])

    # Avoid division by zero
    if np.linalg.norm(topic1_vector) == 0 or np.linalg.norm(topic2_vector) == 0:
        return 0

    return np.dot(topic1_vector, topic2_vector) / (np.linalg.norm(topic1_vector) * np.linalg.norm(topic2_vector))

def preprocess_text(text):
    tokens = word_tokenize(text)
    stop_words = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()

    # Lemmatization, stopword removal, and POS filtering
    filtered_tokens = [lemmatizer.lemmatize(word.lower()) for word in tokens 
                       if word.lower() not in stop_words 
                       and word.isalpha()]
    pos_tags = nltk.pos_tag(filtered_tokens)
    return [word for word, pos in pos_tags if pos.startswith('N') or pos.startswith('J')]

processed_texts = [preprocess_text(text) for text in X]

dictionary = corpora.Dictionary(processed_texts)
doc_term_matrix = [dictionary.doc2bow(text) for text in processed_texts]

lda_model = build_lda_model(doc_term_matrix, dictionary, num_topics=5)
topics = lda_model.print_topics(num_words=25)

for topic in topics:
    print(topic)









